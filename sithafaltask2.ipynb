{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMdg/BQjf2ltru//omSSE+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WF9FWWCaiJea","executionInfo":{"status":"ok","timestamp":1734589198451,"user_tz":-330,"elapsed":56181,"user":{"displayName":"Titoreddy Annapureddy","userId":"04611483658771598309"}},"outputId":"c22fed21-b905-46c9-f727-6794a0747052"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the website URLs (comma-separated): https://www.icc-cricket.com/tournaments/t20cricketworldcup\n","Starting to scrape websites...\n","Scraping: https://www.icc-cricket.com/tournaments/t20cricketworldcup\n","Splitting text into chunks...\n","Generating embeddings for text chunks...\n","Saving the FAISS index...\n","FAISS index saved.\n","\n","Ask a Question (or type 'exit' to quit): mention some indian cricket players\n","\n","Answer:\n","Based on the provided context, some Indian cricket players mentioned are:\n","\n","1. Rohit Sharma\n","2. Virat Kohli\n","3. Jasprit Bumrah\n","4. Hardik Pandya\n","5. Suryakumar Yadav\n","6. Axar Patel\n","\n","Ask a Question (or type 'exit' to quit): classify their roles in cricket\n","\n","Answer:\n","Based on the provided context, the roles of the mentioned individuals in cricket are:\n","\n","1. Rohit Sharma - Batsman and former captain of the Indian cricket team.\n","2. Virat Kohli - Batsman.\n","3. Axar Patel - All-rounder (batsman and bowler).\n","4. Jasprit Bumrah - Bowler.\n","5. Hardik Pandya - All-rounder (batsman and bowler).\n","6. Gautam Gambhir - Former batsman and current head coach of the Indian cricket team.\n","7. Suryakumar Yadav - Batsman.\n","8. Rahul Dravid - Former head coach of the Indian cricket team.\n","9. Starc - Bowler (Australian cricket team).\n","10. Ponting - Former batsman and coach (Australian cricket team).\n","11. Gambhir - Former batsman and current head coach of the Indian cricket team.\n","\n","Ask a Question (or type 'exit' to quit): exit\n","Exiting the query system.\n"]}],"source":["import os\n","import pickle\n","import time\n","import requests\n","from bs4 import BeautifulSoup\n","from langchain_groq import ChatGroq\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.chains import RetrievalQA\n","\n","# Initialize LLM (Groq model)\n","llm = ChatGroq(temperature=0, groq_api_key=\"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\", model_name=\"llama-3.1-70b-versatile\")\n","\n","# File path for FAISS index\n","file_path = \"faiss_store_openai.pkl\"\n","\n","# Function to scrape content from a website\n","def scrape_website(url):\n","    try:\n","        response = requests.get(url)\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.content, 'html.parser')\n","\n","            # Extract text from specific HTML sections (e.g., <article>, <section>, etc.)\n","            main_content = soup.find_all(['article', 'section', 'div', 'header', 'footer'])\n","            text = ' '.join([element.get_text() for element in main_content])\n","\n","            return text.strip()\n","        else:\n","            print(f\"Failed to retrieve {url}\")\n","            return \"\"\n","    except Exception as e:\n","        print(f\"Error scraping {url}: {e}\")\n","        return \"\"\n","\n","# Function to process website content and save embeddings to FAISS\n","def process_websites(urls):\n","    all_text = \"\"\n","\n","    print(\"Starting to scrape websites...\")\n","    for url in urls:\n","        print(f\"Scraping: {url}\")\n","        extracted_text = scrape_website(url)\n","        all_text += extracted_text + \"\\n\"\n","\n","    # Split text into smaller chunks\n","    print(\"Splitting text into chunks...\")\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n","    text_chunks = text_splitter.split_text(all_text)\n","\n","    # Create embeddings using HuggingFace model\n","    print(\"Generating embeddings for text chunks...\")\n","    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","    vectorstore = FAISS.from_texts(text_chunks, embeddings)\n","\n","    # Save the FAISS index to a pickle file\n","    print(\"Saving the FAISS index...\")\n","    with open(file_path, \"wb\") as f:\n","        pickle.dump(vectorstore, f)\n","    print(\"FAISS index saved.\")\n","\n","# Function to query the processed data\n","def handle_query():\n","    while True:\n","        query = input(\"\\nAsk a Question (or type 'exit' to quit): \").strip()\n","        if query.lower() == 'exit':\n","            print(\"Exiting the query system.\")\n","            break\n","\n","        if query:\n","            try:\n","                # Load the FAISS index from the pickle file\n","                if os.path.exists(file_path):\n","                    with open(file_path, \"rb\") as f:\n","                        vectorstore = pickle.load(f)\n","\n","                    # Initialize the retrieval chain\n","                    chain = RetrievalQA.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n","\n","                    # Get response from the chain\n","                    result = chain.run(query)\n","\n","                    # Display the response\n","                    print(\"\\nAnswer:\")\n","                    print(result)\n","                else:\n","                    print(\"FAISS index not found. Please process the websites first.\")\n","            except Exception as e:\n","                print(f\"Error occurred: {e}\")\n","        else:\n","            print(\"Please enter a valid query.\")\n","\n","# Function to handle website URL input and initiate processing\n","def main():\n","    urls = input(\"Enter the website URLs (comma-separated): \").split(',')\n","\n","    # Process websites after URL input\n","    process_websites(urls)\n","\n","    # Start querying in a loop\n","    handle_query()\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}